{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd21f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api = os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda58a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ML Projects\\LLM - Projects\\FAQ_llm\\faq_llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "llm = GooglePalm(google_api_key= api, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc25e0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The use of huggingface instrucator embeddings is to provide a way to represent text data in a way that can be used for machine learning tasks. This can be done by using a language model to learn the embeddings of words and phrases in a text corpus. The embeddings can then be used as features for a variety of machine learning tasks, such as text classification, natural language inference, and question answering.\n",
      "\n",
      "Huggingface instrucator embeddings are particularly useful for tasks that require understanding the meaning of text. This is because the embeddings are learned from a large corpus of text, which means that they capture the semantic relationships between words and phrases. This makes them more effective than other types of text representations, such as bag-of-words or TF-IDF, for tasks that require understanding the meaning of text.\n",
      "\n",
      "Here are some of the benefits of using huggingface instrucator embeddings:\n",
      "\n",
      "* They are learned from a large corpus of text, which means that they capture the semantic relationships between words and phrases.\n",
      "* They are vector representations, which means that they can be used with a variety of machine learning algorithms.\n",
      "* They are efficient to compute, which makes them suitable for large-scale tasks.\n",
      "\n",
      "Here are some of the applications of huggingface instrucator embeddings:\n",
      "\n",
      "* Text classification\n",
      "* Natural language inference\n",
      "* Question answering\n",
      "* Machine translation\n",
      "* Information retrieval\n",
      "* Spam filtering\n",
      "* Speech recognition\n",
      "\n",
      "If you are working on a machine learning task that requires understanding the meaning of text, then you should consider using huggingface instrucator embeddings.\n"
     ]
    }
   ],
   "source": [
    "print(llm('what is the use of huggingface instrucatorembeddings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d656f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e2449dd",
   "metadata": {},
   "source": [
    "### We are using CSVLoader to load our csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bdfc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d95947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='codebasics_faqs.csv', source_column='prompt')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5856a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58366359",
   "metadata": {},
   "source": [
    "### Using HuggingFaceInstructorEmbeding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a447d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceInstructEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c701406",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = embedding.embed_query('Upendra Is Hero I dont Know How ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b7b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14260b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00901521",
   "metadata": {},
   "source": [
    "### Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d919cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = FAISS.from_documents(documents= data, embedding= embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1137715",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d38e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb5144",
   "metadata": {},
   "source": [
    "##### now you can see in this it is giving similar answer from csv bt it is not like human interactive so we will use llm to so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd2c7478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='prompt: What is different in this course from thousands of other Power BI courses available online?\\nresponse: Most of the courses available on the internet teach you how to build x & y without any business context and do not prepare you for the real business world. This course is rather an experience in which you will learn how to use Power BI & other non-technical skills to solve a real-life business problem using analytics. Here you focus on solving a business problem and in that process learn how Power BI can be used as a tool. This is how you will do the work when you start working as a data analyst/ Business analyst/ Power BI developer in the industry. This course will prepare you for not just fetching the job but, shine in it & grow further.', metadata={'source': 'What is different in this course from thousands of other Power BI courses available online?', 'row': 36}),\n",
       " Document(page_content='prompt: I have never done programming and belong to a non-technical background. Can I take this course?\\nresponse: Yes, this is the perfect course for anyone who has never done coding and wants to build a career in the IT/Data Analytics industry or just wants to perform better in their current job or business using data.', metadata={'source': 'I have never done programming and belong to a non-technical background. Can I take this course?', 'row': 24}),\n",
       " Document(page_content='prompt: Is there any prerequisite for taking this bootcamp ?\\nresponse: Our bootcamp is specifically designed for beginners with no prior experience in this field. The only prerequisite is that you need to have a functional laptop with at least 4GB ram, an internet connection, and a thrill to learn data analysis.', metadata={'source': 'Is there any prerequisite for taking this bootcamp ?', 'row': 2}),\n",
       " Document(page_content='prompt: What is the duration of this bootcamp? How long will it last?\\nresponse: You can complete all courses in 3 months if you dedicate 2-3 hours per day.', metadata={'source': 'What is the duration of this bootcamp? How long will it last?', 'row': 8})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"what is the duration of data analytics course and how about internship?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8aa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c23368d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74dbf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm, \n",
    "                                 retriever= retriever, \n",
    "                                 input_key= \"query\", \n",
    "                                 return_source_documents= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22ef27ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 months internship after course'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"what is the duration of data analytics course and how about internship?\").get('result')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6cdcd",
   "metadata": {},
   "source": [
    "#### see in this llm giving answers for two different questions without changing the meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27cc7ca",
   "metadata": {},
   "source": [
    "now we will give some propmts coz instead of using the given information llm also use its own genral knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "382b1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57211cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
    "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5728494",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template= prompt_template, input_variables=['context','question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f05e69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm, \n",
    "                                 retriever= retriever, \n",
    "                                 input_key= \"query\", \n",
    "                                 return_source_documents= True,chain_type_kwargs={\"prompt\":prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7f3d1",
   "metadata": {},
   "source": [
    "## Now we are deploying it using Streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be2768ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? response: I don't have any cola.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa('can i have some cola').get('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f6bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faq_llm",
   "language": "python",
   "name": "faq_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
